(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{191:function(e,o,t){"use strict";t.r(o);var a=t(0),n=Object(a.a)({},function(){var e=this,o=e.$createElement,t=e._self._c||o;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"aws-glossary"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#aws-glossary","aria-hidden":"true"}},[e._v("#")]),e._v(" AWS Glossary")]),e._v(" "),t("p",[e._v("This page provides a basic reference to cloud computing concepts and AWS terminology, with links to the official AWS documentation provided.")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("S3 Buckets")]),e._v(": "),t("a",{attrs:{href:"https://aws.amazon.com/s3/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Amazon Simple Storage Service (S3)"),t("OutboundLink")],1),e._v(" is designed for data storage in the cloud. Users create an S3 bucket within a specific AWS Region via the "),t("a",{attrs:{href:"https://aws.amazon.com/cli/",target:"_blank",rel:"noopener noreferrer"}},[e._v("AWS Command Line Interface (CLI)"),t("OutboundLink")],1),e._v(" or the "),t("a",{attrs:{href:"https://aws.amazon.com/console/",target:"_blank",rel:"noopener noreferrer"}},[e._v("AWS Management Console"),t("OutboundLink")],1),e._v(" web interface. This location is then used much like a standard subdirectory to upload, store, and download data.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("EC2 Instances")]),e._v(": "),t("a",{attrs:{href:"https://aws.amazon.com/ec2/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Amazon Elastic Compute Cloud (Amazon EC2)"),t("OutboundLink")],1),e._v(" provides the computational resource for AWS users, which come in a variety of instance sizes, operating systems, number of cores, and prices.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("EBS Volumes")]),e._v(": "),t("a",{attrs:{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Amazon Elastic Block Store (Amazon EBS)"),t("OutboundLink")],1),e._v(" is the storage volumes used for EC2 instances for computation.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("AWS Batch")]),e._v(": "),t("a",{attrs:{href:"https://aws.amazon.com/batch/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Amazon Batch"),t("OutboundLink")],1),e._v(" dynamically provisions the optimal quantity and type of compute resources (both Spot or On-Demand) necessary for analysis. It functions analogously to a relatively simple job scheduler used by HPC environments (e.g. LSF or SGE) to schedule and execute batch computing workloads. You can read this "),t("a",{attrs:{href:"https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/",target:"_blank",rel:"noopener noreferrer"}},[e._v("blog post"),t("OutboundLink")],1),e._v(" for more information related to using AWS Batch for running Genomics Workflows.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("EBS Autoscaling")]),e._v(": "),t("a",{attrs:{href:"https://docs.opendata.aws/genomics-workflows/core-env/create-custom-compute-resources/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Autoscalling EBS"),t("OutboundLink")],1),e._v(" EC2 instances have a running process which monitors disk usage and add more EBS volumes on the fly to expand the free space based on the capacity threshold. This feature increass in volume size, adjustments to performance, or changes in the volume type while the volume is in use, e.g. the disk size will increase automatically while processes are running.")])])])])},[],!1,null,null,null);o.default=n.exports}}]);